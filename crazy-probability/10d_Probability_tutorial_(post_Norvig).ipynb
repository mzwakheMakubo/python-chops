{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "10d_Probability tutorial (post-Norvig).ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugw4oT9-zsoE",
        "colab_type": "text"
      },
      "source": [
        "# Probability\n",
        "## A supplement to the Norvig tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWXEGhgSzsoF",
        "colab_type": "text"
      },
      "source": [
        "Well done! You've got through Peter Norvig's introduction to probability, hopefully following the advice given beforehand, and managed to understand the basics of probability theory (maybe you even managed to figure out a little bit of Python as well... lots more of that to come soon!). There are just a few aspects of probability that don't get as full an airing as we would like in Norvig, so the aim of this supplementary tutorial is to riff a little on those themes. We're going to be looking a little deeper at:\n",
        "\n",
        "* Kolmogorov's axioms of probability: putting probability theory on a solid theoretical foundation\n",
        "\n",
        "* some theorems that arise from those axioms\n",
        "\n",
        "* conditional probability\n",
        "\n",
        "* the concepts of dependence and independence\n",
        "    \n",
        "What are we waiting for? Let's get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK5NQ4vpzsoG",
        "colab_type": "text"
      },
      "source": [
        "### Kolmogorov's axioms of probability\n",
        "\n",
        "Probability theory was put on a solid theoretical foundation in the 20th century by Andrey Kolmogorov, a briliant Soviet mathematician. Here is the man himself, making notes for a lecture in Tallin in 1973 (to which we've of course added a twist by putting the picture through [the Deep Dream Generator](https://deepdreamgenerator.com)).\n",
        "\n",
        "<img src = \"https://github.com/Explore-AI/Pictures/blob/master/kolmogorov_dd.jpg?raw=true\" width = 400px>\n",
        "\n",
        "We're going to try and avoid too much formal mathematical logic here, and focus on the intuition. There are essentially three axioms of probabiity, taking a sample space $S$ consisting of a number of events which we'll denote $A$:\n",
        "\n",
        "1. The probability of any given event is a non-negative real number: $P(A) \\ge 0 \\forall A \\in S$.\n",
        "\n",
        "2. The probability of the sample space, $S$, which is all possible events, is unity: $P(S) = 1$. \n",
        "\n",
        "3. If we subdivide some elements of $S$ into a set $B$ and other into another set $C$, such that $B$ and $C$ are mutually exclusive (have no common elements), then the probability of the union of these sets is equal to the sum of their probabilities: $P(B \\cup C) = P(B) + P(C)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIxkCvECzsoH",
        "colab_type": "text"
      },
      "source": [
        "### Implications of these axioms\n",
        "\n",
        "There are some neat implications which follow directly from the axioms above, as follows:\n",
        "\n",
        "1. **Upper bound**: axioms 1 and 2, taken together,  effectively imply an upper bound of 1 on any individual event.\n",
        "\n",
        "2. **The probability of the empty set is zero**: $P(\\emptyset) = 0$.\n",
        "\n",
        "3. **Monotonicity**: if $A$ is a subset of $B$, then its probability must be less than or equal to that of $B$: $A \\subset B \\implies P(A) \\le P(B)$.\n",
        "\n",
        "4. **Sum rule**: the probability of $A$ or $B$ occurring is equal to the sum of their respective probabilities, less the probability of both occurring: $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$.\n",
        "\n",
        "5. **Complement rule**: the probability of the complement of an event $A$ (i.e. all events other than $A$) is 1 mius the probability of $A$: $P(A') = 1 - P(A)$.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtKp-km7zsoI",
        "colab_type": "text"
      },
      "source": [
        "### Conditional probability and Bayes' Theorem\n",
        "\n",
        "You got a bit of an introduction to the concept of conditional probability in the Norvig tutorial, in the form of a (sort of) introduction to Bayes' Theorem. The probability of $A$ conditional on $B$, denoted $P(A|B)$, does just what it says on the tin: it represents the probability that $A$ will occur *given that B occurs*. Suppose that I have a standard deck of cards. If I draw a card at random, the probability that it will be an ace is 1 in 13 (there are 4 aces in the deck, and 52 cards in total). But what if I have drawn out two cards already, and both were aces? Now the probability of drawing out an ace with the next card is a conditional probability: the probabilty of an ace conditional on two aces having already been drawn. In this case, a little logic will inform us that there are now 50 cards left in the deck, only two of which are aces, so the conditional probability we're after is 1 in 25.\n",
        "\n",
        "Conditional probability problems are common and important in the real world of data science, and it's worth making sure that you understand them, and in particular understand Bayes' Theorem, which we can write as follows:\n",
        "\n",
        "$$P(A|B) = \\displaystyle \\frac{P(B|A).P(A)}{P(B)}.$$\n",
        "\n",
        "This is an incredibly useful rule which allows us to compute conditional probabilities we wouldn't otherwise be able to by considering the conditioning in the other direction, together with the unconditional probabilities of the two events in question.\n",
        "\n",
        "Suppose that your project teammate Bob likes to enjoy himself, maybe a little too much, on weekends; there's a 10% chance that he'll go out and overdo it. If he does, there's a 45% chance that he won't make it into the project briefing at 9am on a Monday morning. You also estimate that he misses 1 in every 20 Monday morning meetings, so there's a 5% chance of this occurring. Here you sit on Monday and Bob hasn't turned up in time, and you wonder to yourself, what is the probability that he overdid it this past weekend?\n",
        "\n",
        "Let's ask the Rev. Thomas Bayes. We'll call $A$ the event of Bob overdoing it on the weekend, and $B$ the event of him missing the Monday morning session. So we have that $P(A) = 0.1, P(B) = 0.05$ and $P(B|A)$ (the probability that he misses the Monday meeting conditional on having had too big a weekend) = 0.45. We're interested in $P(A|B)$, the probability that he overdid things conditional on him now missing the Monday morning session, which we can evaluate as:\n",
        "\n",
        "$$P(A|B) = \\displaystyle \\frac{P(B|A).P(A)}{P(B)} = \\frac{0.45 \\times 0.1}{0.05} = 0.9.$$\n",
        "\n",
        "So you can safely conclude that there's a 90% chance that Bob overdid things this past weekend.\n",
        "\n",
        "Let's think about the intuition here. On 10% of weekends Bob goes at it too hard, and on 45% of the following Monday mornings he's unable to get out of bed in time for the briefing. So that means 4.5% of his Mondays follow this pattern. We know that he fails to make it on 5% of his Mondays, so it's easy to see that a heavy weekend accounts for 90% of the cases where he fails to make it on Monday morning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLZx7ivAzsoJ",
        "colab_type": "text"
      },
      "source": [
        "### Dependence and independence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1THUPpU0zsoJ",
        "colab_type": "text"
      },
      "source": [
        "The final concept to unpack in this short tutorial is that of independence. Avoiding mathematical formality, when we say that $A$ is independent of $B$ in probability what we mean is that the outcome of $A$ does not depend in any way on the outcome of $B$. So Manchester United winning the Premier League is independent of the volume of Norwegian salmon sold in Scotland (or so we think, anyway... have to confess I don't actually have the data on this). But Manchester United winning the Premier League is almost certainly not independent of (and is therefore, in a statistical sense, dependent on) the amount of money spent on players by Chelsea and Manchester City.\n",
        "\n",
        "In probability theory, if $A$ and $B$ are independent then this implies that:\n",
        "\n",
        "$$P(A \\cap B) = P(A).P(B),$$\n",
        "\n",
        "in other words the probability of both $A$ and $B$ occurring is the product of their individual probabilities. This implies also that $P(A) = P(A|B)$: the probability of $A$ is the same as the probability of $A$ conditional on $B$, i.e. the outcome of $B$ has no effect on $A$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzPX0tE0zsoK",
        "colab_type": "text"
      },
      "source": [
        "### That's a wrap for now!\n",
        "\n",
        "Well done. Now's a good time to take a breath, and then go back over the probability material and make sure you've bedded it down properly, as it's an important foundation for the statistics we're going to learn over the next few weeks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA3aw7f2zsoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}